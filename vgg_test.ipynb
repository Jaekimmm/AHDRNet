{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGFeatureExtractor, self).__init__()\n",
    "        \n",
    "        vgg = models.vgg16(pretrained=True).features.eval()\n",
    "        \n",
    "        self.selected_layers = [ 0,  1, # block1_conv1 + relu\n",
    "                                 5,  6, # block2_conv1 + relu\n",
    "                                10, 11, # block3_conv1 + relu\n",
    "                                17, 18, # block4_conv1 + relu\n",
    "                                24, 25] # block5_conv1 + relu\n",
    "        \n",
    "        self.layers = nn.ModuleList([vgg[idx] for idx in self.selected_layers])\n",
    "        \n",
    "        for param in self.layers.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def forward(self, x):\n",
    "        outputs = {}\n",
    "        for i, layer in zip(self.selected_layers, self.layers):\n",
    "            x = layer(x)\n",
    "            outputs[f\"layer_{i}\"] = x\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_model = VGGFeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_info(model, input_size=(1, 3, 224, 224)):\n",
    "    \"\"\" PyTorch 모델의 모든 Layer 정보를 출력 \"\"\"\n",
    "    \n",
    "    print(\"--------------------------------------------------------------------------------\")\n",
    "    print(f\"{'Layer Name':<30}{'Type':<30}{'Output Shape':<30}{'Activation':<20}{'Trainable'}\")\n",
    "    print(\"=\"*110)\n",
    "\n",
    "    # 가상의 입력을 생성하여 Forward 수행\n",
    "    x = torch.randn(*input_size).to(next(model.parameters()).device)\n",
    "\n",
    "    for name, layer in model.named_children():\n",
    "        x = layer(x)  # Forward pass\n",
    "        \n",
    "        # ✅ 활성화 함수(Activation) 확인\n",
    "        activation = \"None\"\n",
    "        if isinstance(layer, nn.ReLU):\n",
    "            activation = \"ReLU\"\n",
    "        elif isinstance(layer, nn.Sigmoid):\n",
    "            activation = \"Sigmoid\"\n",
    "        elif isinstance(layer, nn.Tanh):\n",
    "            activation = \"Tanh\"\n",
    "\n",
    "        # ✅ Layer 정보 출력\n",
    "        print(f\"{name:<30}{layer.__class__.__name__:<30}{str(tuple(x.shape)):<30}{activation:<20}{layer.requires_grad}\")\n",
    "\n",
    "    print(\"=\"*110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_info(model, input_size=(1, 3, 256, 256)):\n",
    "    \"\"\" PyTorch 모델의 모든 Layer 정보를 출력 (ModuleList 지원) \"\"\"\n",
    "    \n",
    "    print(\"--------------------------------------------------------------------------------\")\n",
    "    print(f\"{'Layer Name':<30}{'Type':<30}{'Output Shape':<30}{'Activation':<20}{'Trainable'}\")\n",
    "    print(\"=\"*110)\n",
    "\n",
    "    # 가상의 입력을 생성하여 Forward 수행\n",
    "    x = torch.randn(*input_size).to(next(model.parameters()).device)\n",
    "\n",
    "    for name, layer in model.named_children():\n",
    "        # ✅ ModuleList인 경우 개별 Layer 순회\n",
    "        if isinstance(layer, nn.ModuleList):\n",
    "            for sub_idx, sub_layer in enumerate(layer):\n",
    "                x = sub_layer(x)  # ✅ 개별 Layer 실행\n",
    "                activation = get_activation(sub_layer)\n",
    "                print(f\"{f'{name}[{sub_idx}] ({sub_layer.__class__.__name__})':<30}\"\n",
    "                      f\"{str(tuple(x.shape)):<30}\"\n",
    "                      f\"{activation:<20}\")\n",
    "        else:\n",
    "            x = layer(x)  # ✅ 일반 Layer 실행\n",
    "            activation = get_activation(layer)\n",
    "            print(f\"{f'{name} ({layer.__class__.__name__})':<30}\"\n",
    "                  f\"{str(tuple(x.shape)):<30}\"\n",
    "                  f\"{activation:<20}\")\n",
    "\n",
    "    print(\"=\"*110)\n",
    "\n",
    "def get_activation(layer):\n",
    "    \"\"\" Layer에서 Activation 함수 찾기 \"\"\"\n",
    "    if isinstance(layer, nn.ReLU):\n",
    "        return \"ReLU\"\n",
    "    elif isinstance(layer, nn.Sigmoid):\n",
    "        return \"Sigmoid\"\n",
    "    elif isinstance(layer, nn.Tanh):\n",
    "        return \"Tanh\"\n",
    "    return \"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Layer Name                    Type                          Output Shape                  Activation          Trainable\n",
      "==============================================================================================================\n",
      "layers[0] (Conv2d)            (1, 64, 256, 256)             None                \n",
      "layers[1] (ReLU)              (1, 64, 256, 256)             ReLU                \n",
      "layers[2] (Conv2d)            (1, 128, 256, 256)            None                \n",
      "layers[3] (ReLU)              (1, 128, 256, 256)            ReLU                \n",
      "layers[4] (Conv2d)            (1, 256, 256, 256)            None                \n",
      "layers[5] (ReLU)              (1, 256, 256, 256)            ReLU                \n",
      "layers[6] (Conv2d)            (1, 512, 256, 256)            None                \n",
      "layers[7] (ReLU)              (1, 512, 256, 256)            ReLU                \n",
      "layers[8] (Conv2d)            (1, 512, 256, 256)            None                \n",
      "layers[9] (ReLU)              (1, 512, 256, 256)            ReLU                \n",
      "==============================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print_model_info(loss_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vgg_full = models.vgg16(weights='DEFAULT').features.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Layer Name                    Type                          Output Shape                  Activation          Trainable\n",
      "==============================================================================================================\n",
      "0 (Conv2d)                    (1, 64, 256, 256)             None                \n",
      "1 (ReLU)                      (1, 64, 256, 256)             ReLU                \n",
      "2 (Conv2d)                    (1, 64, 256, 256)             None                \n",
      "3 (ReLU)                      (1, 64, 256, 256)             ReLU                \n",
      "4 (MaxPool2d)                 (1, 64, 128, 128)             None                \n",
      "5 (Conv2d)                    (1, 128, 128, 128)            None                \n",
      "6 (ReLU)                      (1, 128, 128, 128)            ReLU                \n",
      "7 (Conv2d)                    (1, 128, 128, 128)            None                \n",
      "8 (ReLU)                      (1, 128, 128, 128)            ReLU                \n",
      "9 (MaxPool2d)                 (1, 128, 64, 64)              None                \n",
      "10 (Conv2d)                   (1, 256, 64, 64)              None                \n",
      "11 (ReLU)                     (1, 256, 64, 64)              ReLU                \n",
      "12 (Conv2d)                   (1, 256, 64, 64)              None                \n",
      "13 (ReLU)                     (1, 256, 64, 64)              ReLU                \n",
      "14 (Conv2d)                   (1, 256, 64, 64)              None                \n",
      "15 (ReLU)                     (1, 256, 64, 64)              ReLU                \n",
      "16 (MaxPool2d)                (1, 256, 32, 32)              None                \n",
      "17 (Conv2d)                   (1, 512, 32, 32)              None                \n",
      "18 (ReLU)                     (1, 512, 32, 32)              ReLU                \n",
      "19 (Conv2d)                   (1, 512, 32, 32)              None                \n",
      "20 (ReLU)                     (1, 512, 32, 32)              ReLU                \n",
      "21 (Conv2d)                   (1, 512, 32, 32)              None                \n",
      "22 (ReLU)                     (1, 512, 32, 32)              ReLU                \n",
      "23 (MaxPool2d)                (1, 512, 16, 16)              None                \n",
      "24 (Conv2d)                   (1, 512, 16, 16)              None                \n",
      "25 (ReLU)                     (1, 512, 16, 16)              ReLU                \n",
      "26 (Conv2d)                   (1, 512, 16, 16)              None                \n",
      "27 (ReLU)                     (1, 512, 16, 16)              ReLU                \n",
      "28 (Conv2d)                   (1, 512, 16, 16)              None                \n",
      "29 (ReLU)                     (1, 512, 16, 16)              ReLU                \n",
      "30 (MaxPool2d)                (1, 512, 8, 8)                None                \n",
      "==============================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print_model_info(vgg_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGFeatureExtractor, self).__init__()\n",
    "        \n",
    "        vgg = models.vgg16(weights='DEFAULT').features.eval()\n",
    "        self.layers = nn.ModuleList(vgg[:25+1])\n",
    "        for param in self.layers.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def forward(self, x):\n",
    "        outputs = {}\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            outputs[f\"layer_{i}\"] = x\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Layer Name                    Type                          Output Shape                  Activation          Trainable\n",
      "==============================================================================================================\n",
      "layers[0] (Conv2d)            (1, 64, 256, 256)             None                \n",
      "layers[1] (ReLU)              (1, 64, 256, 256)             ReLU                \n",
      "layers[2] (Conv2d)            (1, 64, 256, 256)             None                \n",
      "layers[3] (ReLU)              (1, 64, 256, 256)             ReLU                \n",
      "layers[4] (MaxPool2d)         (1, 64, 128, 128)             None                \n",
      "layers[5] (Conv2d)            (1, 128, 128, 128)            None                \n",
      "layers[6] (ReLU)              (1, 128, 128, 128)            ReLU                \n",
      "layers[7] (Conv2d)            (1, 128, 128, 128)            None                \n",
      "layers[8] (ReLU)              (1, 128, 128, 128)            ReLU                \n",
      "layers[9] (MaxPool2d)         (1, 128, 64, 64)              None                \n",
      "layers[10] (Conv2d)           (1, 256, 64, 64)              None                \n",
      "layers[11] (ReLU)             (1, 256, 64, 64)              ReLU                \n",
      "layers[12] (Conv2d)           (1, 256, 64, 64)              None                \n",
      "layers[13] (ReLU)             (1, 256, 64, 64)              ReLU                \n",
      "layers[14] (Conv2d)           (1, 256, 64, 64)              None                \n",
      "layers[15] (ReLU)             (1, 256, 64, 64)              ReLU                \n",
      "layers[16] (MaxPool2d)        (1, 256, 32, 32)              None                \n",
      "layers[17] (Conv2d)           (1, 512, 32, 32)              None                \n",
      "layers[18] (ReLU)             (1, 512, 32, 32)              ReLU                \n",
      "layers[19] (Conv2d)           (1, 512, 32, 32)              None                \n",
      "layers[20] (ReLU)             (1, 512, 32, 32)              ReLU                \n",
      "layers[21] (Conv2d)           (1, 512, 32, 32)              None                \n",
      "layers[22] (ReLU)             (1, 512, 32, 32)              ReLU                \n",
      "layers[23] (MaxPool2d)        (1, 512, 16, 16)              None                \n",
      "layers[24] (Conv2d)           (1, 512, 16, 16)              None                \n",
      "layers[25] (ReLU)             (1, 512, 16, 16)              ReLU                \n",
      "==============================================================================================================\n"
     ]
    }
   ],
   "source": [
    "loss_model_new = VGGFeatureExtractor()\n",
    "print_model_info(loss_model_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ahdr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
